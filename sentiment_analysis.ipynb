{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:20:50.988676Z",
     "start_time": "2025-11-23T06:20:32.846817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"tensorflow-macos==2.16.2\" \"tensorflow-metal\"\n",
    "!pip install \"tf-keras==2.16.0\"\n",
    "!pip install transformers pandas scikit-learn"
   ],
   "id": "c98756bedd3a073c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.10/site-packages (25.3)\r\n",
      "Requirement already satisfied: tensorflow-macos==2.16.2 in ./.venv/lib/python3.10/site-packages (2.16.2)\r\n",
      "Requirement already satisfied: tensorflow-metal in ./.venv/lib/python3.10/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: tensorflow==2.16.2 in ./.venv/lib/python3.10/site-packages (from tensorflow-macos==2.16.2) (2.16.2)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.3.1)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (25.9.23)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.15.1)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (18.1.1)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.3.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (25.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (4.25.8)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.32.5)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (80.9.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (4.15.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.0.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.76.0)\r\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.16.2)\r\n",
      "Requirement already satisfied: keras>=3.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.12.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.37.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./.venv/lib/python3.10/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.26.4)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2025.11.12)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.10)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.1.3)\r\n",
      "Requirement already satisfied: wheel~=0.35 in ./.venv/lib/python3.10/site-packages (from tensorflow-metal) (0.41.2)\r\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (14.2.0)\r\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.1.0)\r\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.18.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.0.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.19.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.1.2)\r\n",
      "Requirement already satisfied: tf-keras==2.16.0 in ./.venv/lib/python3.10/site-packages (2.16.0)\r\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in ./.venv/lib/python3.10/site-packages (from tf-keras==2.16.0) (2.16.2)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2.3.1)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (25.9.23)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.15.1)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (18.1.1)\r\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.3.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.4.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (25.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (4.25.8)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2.32.5)\r\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (80.9.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (1.17.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (4.15.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2.0.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (1.76.0)\r\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2.16.2)\r\n",
      "Requirement already satisfied: keras>=3.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.12.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.37.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras==2.16.0) (1.26.4)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2025.11.12)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.10)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.1.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.41.2)\r\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (14.2.0)\r\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.1.0)\r\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.18.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (3.0.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (2.19.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras==2.16.0) (0.1.2)\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.57.1)\r\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.7.2)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.20.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.36.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2025.11.3)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.11.12)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:34.914007Z",
     "start_time": "2025-11-23T06:20:50.995898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# IMPORTS & BASIC CONFIG\n",
    "# ===========================\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras  # Keras compatibility layer for TensorFlow 2.16+\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility (set seeds)\n",
    "# ---------------------------\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ---------------------------\n",
    "# Model / training parameters\n",
    "# ---------------------------\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Pretrained Transformer\n",
    "MAX_LENGTH = 64                        # Max token length per phrase\n",
    "BATCH_SIZE = 8                         # Batch size\n",
    "EPOCHS = 3                              # Number of training passes\n",
    "LEARNING_RATE = 5e-5                    # Fine-tuning learning rate\n",
    "\n",
    "NUM_LABELS = 5  # 0..4 sentiments in this competition\n",
    "\n",
    "# ---------------------------\n",
    "# File paths for Kaggle data\n",
    "# ---------------------------\n",
    "# Make sure train.tsv and test.tsv are in the same folder as your notebook.\n",
    "TRAIN_PATH = \"train.tsv\"\n",
    "TEST_PATH = \"test.tsv\"\n",
    "\n",
    "# ---------------------------\n",
    "# Human-readable label names\n",
    "# ---------------------------\n",
    "\n",
    "id2label = {\n",
    "    0: \"negative\",\n",
    "    1: \"somewhat negative\",\n",
    "    2: \"neutral\",\n",
    "    3: \"somewhat positive\",\n",
    "    4: \"positive\",\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(\"Python executable:\", os.sys.executable)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tf_keras version:\", tf_keras.__version__)\n"
   ],
   "id": "abf69ac882ed6ff0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianasotir/ai-agents-workshop/sentiment_model_terraform_transformers/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /Users/dianasotir/ai-agents-workshop/sentiment_model_terraform_transformers/.venv/bin/python\n",
      "TensorFlow version: 2.16.2\n",
      "tf_keras version: 2.16.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:35.185753Z",
     "start_time": "2025-11-23T06:21:35.182773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# CHECK DEVICES\n",
    "# ===========================\n",
    "\n",
    "print(\"Available devices:\")\n",
    "for d in tf.config.list_physical_devices():\n",
    "    print(\"  -\", d)\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    print(\"\\n‚úÖ GPU/Metal device detected ‚Äì training will be faster.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No GPU detected ‚Äì training will run on CPU (still fine for this project).\")\n"
   ],
   "id": "867f12f75001c8f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "  - PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "  - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      "‚úÖ GPU/Metal device detected ‚Äì training will be faster.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:35.203831Z",
     "start_time": "2025-11-23T06:21:35.201427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Disable all GPUs ‚Äì force CPU-only\n",
    "try:\n",
    "    tf.config.set_visible_devices([], \"GPU\")\n",
    "except Exception as e:\n",
    "    print(\"Could not disable GPU devices:\", e)\n",
    "\n",
    "print(\"After disabling GPUs:\", tf.config.list_physical_devices())"
   ],
   "id": "dcf9fb8e687a1814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After disabling GPUs: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:35.334064Z",
     "start_time": "2025-11-23T06:21:35.220169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# LOAD TRAIN DATA\n",
    "# ===========================\n",
    "# train.tsv columns (Kaggle Rotten Tomatoes):\n",
    "#   - PhraseId\n",
    "#   - SentenceId\n",
    "#   - Phrase      (text)\n",
    "#   - Sentiment   (0..4)\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH, sep=\"\\t\")\n",
    "print(\"Train data shape:\", df.shape)\n",
    "df.head()\n"
   ],
   "id": "b25f90f3c003f6c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (156060, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:35.365279Z",
     "start_time": "2025-11-23T06:21:35.361064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# BASIC EDA: LABEL COUNTS\n",
    "# ===========================\n",
    "\n",
    "label_counts = df[\"Sentiment\"].value_counts().sort_index()\n",
    "print(\"Label counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "print(\"\\nLabel proportions:\")\n",
    "print((label_counts / len(df)).round(3))\n"
   ],
   "id": "d8ff07ef3f9f4b3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "Sentiment\n",
      "0     7072\n",
      "1    27273\n",
      "2    79582\n",
      "3    32927\n",
      "4     9206\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label proportions:\n",
      "Sentiment\n",
      "0    0.045\n",
      "1    0.175\n",
      "2    0.510\n",
      "3    0.211\n",
      "4    0.059\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:35.437666Z",
     "start_time": "2025-11-23T06:21:35.398999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# TRAIN / VALIDATION SPLIT\n",
    "# ===========================\n",
    "# We split train.tsv into:\n",
    "#   - training set (90%)\n",
    "#   - validation set (10%)\n",
    "# Using stratify=Sentiment keeps the label distribution similar.\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"Sentiment\"],\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "\n",
    "train_df.head()\n"
   ],
   "id": "53cf5fd0eb65ea58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 140454\n",
      "Validation size: 15606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "116940    116941        6239   \n",
       "52039      52040        2567   \n",
       "88064      88065        4574   \n",
       "4874        4875         188   \n",
       "18448      18449         809   \n",
       "\n",
       "                                                   Phrase  Sentiment  \n",
       "116940                         haunting about `` Fence ''          2  \n",
       "52039                           considered in its details          2  \n",
       "88064                                       three words :          2  \n",
       "4874                                               Uneven          1  \n",
       "18448   liked it more if it had just gone that one ste...          2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116940</th>\n",
       "      <td>116941</td>\n",
       "      <td>6239</td>\n",
       "      <td>haunting about `` Fence ''</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52039</th>\n",
       "      <td>52040</td>\n",
       "      <td>2567</td>\n",
       "      <td>considered in its details</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88064</th>\n",
       "      <td>88065</td>\n",
       "      <td>4574</td>\n",
       "      <td>three words :</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4874</th>\n",
       "      <td>4875</td>\n",
       "      <td>188</td>\n",
       "      <td>Uneven</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18448</th>\n",
       "      <td>18449</td>\n",
       "      <td>809</td>\n",
       "      <td>liked it more if it had just gone that one ste...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:36.186403Z",
     "start_time": "2025-11-23T06:21:36.179986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Temporary reduction of subset\n",
    "# üîπ Debug mode: train on a smaller subset\n",
    "MAX_TRAIN_SAMPLES = 5000\n",
    "MAX_VAL_SAMPLES   = 1000\n",
    "\n",
    "if len(train_df) > MAX_TRAIN_SAMPLES:\n",
    "    train_df = train_df.sample(n=MAX_TRAIN_SAMPLES, random_state=SEED)\n",
    "\n",
    "if len(val_df) > MAX_VAL_SAMPLES:\n",
    "    val_df = val_df.sample(n=MAX_VAL_SAMPLES, random_state=SEED)\n",
    "\n",
    "print(\"Train size (debug):\", len(train_df))\n",
    "print(\"Val size (debug):\", len(val_df))"
   ],
   "id": "331458832d57c161",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (debug): 5000\n",
      "Val size (debug): 1000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:37.218640Z",
     "start_time": "2025-11-23T06:21:36.220328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# LOAD TOKENIZER\n",
    "# ===========================\n",
    "# The tokenizer turns text into:\n",
    "#   - input_ids (token IDs)\n",
    "#   - attention_mask (masking padding tokens)\n",
    "# This *must* match the pretrained model we use.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "example_text = \"This movie was surprisingly good!\"\n",
    "encoded_example = tokenizer(\n",
    "    example_text,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "print(\"Example text:\", example_text)\n",
    "print(\"Tokenizer keys:\", encoded_example.keys())\n",
    "print(\"Length of input_ids:\", len(encoded_example[\"input_ids\"]))\n"
   ],
   "id": "469b3d4bbb3adbff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text: This movie was surprisingly good!\n",
      "Tokenizer keys: KeysView({'input_ids': [101, 2023, 3185, 2001, 10889, 2204, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})\n",
      "Length of input_ids: 64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:37.391437Z",
     "start_time": "2025-11-23T06:21:37.230819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# ENCODING HELPER\n",
    "# ===========================\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize a list/array of texts into model inputs.\n",
    "\n",
    "    Args:\n",
    "        texts: list-like of strings.\n",
    "        tokenizer: Hugging Face tokenizer.\n",
    "        max_length: max token length.\n",
    "\n",
    "    Returns:\n",
    "        dict of NumPy arrays:\n",
    "          {\n",
    "            \"input_ids\": (N, max_length),\n",
    "            \"attention_mask\": (N, max_length)\n",
    "          }\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # pad all sequences to max_length\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"np\",   # return as NumPy arrays\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "\n",
    "# Encode train + validation texts\n",
    "train_texts = train_df[\"Phrase\"].values\n",
    "train_labels = train_df[\"Sentiment\"].values\n",
    "\n",
    "val_texts = val_df[\"Phrase\"].values\n",
    "val_labels = val_df[\"Sentiment\"].values\n",
    "\n",
    "train_encodings = encode_texts(train_texts, tokenizer, max_length=MAX_LENGTH)\n",
    "val_encodings = encode_texts(val_texts, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "print(\"Train input_ids shape:\", train_encodings[\"input_ids\"].shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Val input_ids shape:\", val_encodings[\"input_ids\"].shape)\n",
    "print(\"Val labels shape:\", val_labels.shape)\n"
   ],
   "id": "5024b77eb92cb35e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input_ids shape: (5000, 64)\n",
      "Train labels shape: (5000,)\n",
      "Val input_ids shape: (1000, 64)\n",
      "Val labels shape: (1000,)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:37.426607Z",
     "start_time": "2025-11-23T06:21:37.404637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# BUILD TF.DATA DATASETS\n",
    "# ===========================\n",
    "\n",
    "def make_tf_dataset(encodings, labels, batch_size=32, shuffle=False):\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset from model encodings and labels.\n",
    "\n",
    "    Dataset yields:\n",
    "        ({\"input_ids\": ..., \"attention_mask\": ...}, labels)\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(labels), seed=SEED)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "tf_train_dataset = make_tf_dataset(train_encodings, train_labels,\n",
    "                                   batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "tf_val_dataset = make_tf_dataset(val_encodings, val_labels,\n",
    "                                 batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "tf_train_dataset\n"
   ],
   "id": "a2f87474becee086",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 64), dtype=tf.int64, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:38.503093Z",
     "start_time": "2025-11-23T06:21:37.439947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# LOAD PRETRAINED MODEL\n",
    "# ===========================\n",
    "# We use a pre-trained DistilBERT with a classification head.\n",
    "# num_labels=5 (for our 5 sentiment classes).\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    from_pt=False,          # force loading TF weights\n",
    "    use_safetensors=False,  # avoid safetensors for TF in this environment\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ],
   "id": "3ae8beab45466ab8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66957317 (255.42 MB)\n",
      "Trainable params: 66957317 (255.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T06:21:38.527686Z",
     "start_time": "2025-11-23T06:21:38.517485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# COMPILE MODEL\n",
    "# ===========================\n",
    "# For multi-class classification with integer labels 0..4:\n",
    "#   - Use SparseCategoricalCrossentropy(from_logits=True)\n",
    "#     because the model outputs raw logits, not probabilities.\n",
    "#   - Use accuracy as a basic metric.\n",
    "\n",
    "optimizer = tf_keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_fn = tf_keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=metrics,\n",
    ")\n"
   ],
   "id": "610c30a7ec293f26",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-23T06:21:38.540979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===========================\n",
    "# TRAIN THE MODEL\n",
    "# ===========================\n",
    "\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=EPOCHS,\n",
    ")\n"
   ],
   "id": "c5923c5bd2dfcd55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 377s 571ms/step - loss: 1.0403 - accuracy: 0.5754 - val_loss: 0.9236 - val_accuracy: 0.6430\n",
      "Epoch 2/3\n",
      " 18/625 [..............................] - ETA: 4:48 - loss: 0.6624 - accuracy: 0.7708"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===========================\n",
    "# PLOT TRAINING CURVES\n",
    "# ===========================\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict[\"loss\"], label=\"Train loss\")\n",
    "plt.plot(history_dict[\"val_loss\"], label=\"Val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict[\"accuracy\"], label=\"Train acc\")\n",
    "plt.plot(history_dict[\"val_accuracy\"], label=\"Val acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9c831e632ef64089"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===========================\n",
    "# VALIDATION METRICS (ACCURACY + DETAIL)\n",
    "# ===========================\n",
    "\n",
    "# 1) Basic eval\n",
    "val_loss, val_acc = model.evaluate(tf_val_dataset)\n",
    "print(f\"Validation loss: {val_loss:.4f}\")\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# 2) Get predictions for confusion matrix and classification report\n",
    "#    We'll collect all logits on the validation set.\n",
    "\n",
    "all_val_logits = []\n",
    "all_val_labels = []\n",
    "\n",
    "for batch, labels in tf_val_dataset:\n",
    "    outputs = model(batch)\n",
    "    logits = outputs.logits\n",
    "    all_val_logits.append(logits)\n",
    "    all_val_labels.append(labels)\n",
    "\n",
    "all_val_logits = tf.concat(all_val_logits, axis=0)\n",
    "all_val_labels = tf.concat(all_val_labels, axis=0).numpy()\n",
    "\n",
    "val_pred_ids = tf.argmax(all_val_logits, axis=1).numpy()\n",
    "\n",
    "print(\"Pred shape:\", val_pred_ids.shape)\n"
   ],
   "id": "3c5ffe2a4af562e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===========================\n",
    "# CONFUSION MATRIX & CLASSIFICATION REPORT\n",
    "# ===========================\n",
    "\n",
    "cm = confusion_matrix(all_val_labels, val_pred_ids)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(NUM_LABELS)\n",
    "plt.xticks(tick_marks, [id2label[i] for i in range(NUM_LABELS)], rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, [id2label[i] for i in range(NUM_LABELS)])\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "\n",
    "# Add counts in each cell\n",
    "for i in range(NUM_LABELS):\n",
    "    for j in range(NUM_LABELS):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed metrics\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    all_val_labels,\n",
    "    val_pred_ids,\n",
    "    target_names=[id2label[i] for i in range(NUM_LABELS)],\n",
    "))\n"
   ],
   "id": "6a22176b779d248b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===========================\n",
    "# LOAD TEST DATA\n",
    "# ===========================\n",
    "# test.tsv has:\n",
    "#   PhraseId, SentenceId, Phrase\n",
    "# We need to predict Sentiment for each Phrase.\n",
    "\n",
    "test_df = pd.read_csv(TEST_PATH, sep=\"\\t\")\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "test_df.head()\n"
   ],
   "id": "6defbe4c65d994dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode test phrases\n",
    "test_texts = test_df[\"Phrase\"].values\n",
    "test_encodings = encode_texts(test_texts, tokenizer, max_length=MAX_LENGTH)\n",
    "\n",
    "# Build tf.data dataset without labels\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(dict(test_encodings))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ],
   "id": "46d8be5555270394"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===========================\n",
    "# PREDICT ON TEST & CREATE SUBMISSION\n",
    "# ===========================\n",
    "\n",
    "all_test_logits = []\n",
    "for batch in test_dataset:\n",
    "    outputs = model(batch)\n",
    "    logits = outputs.logits\n",
    "    all_test_logits.append(logits)\n",
    "\n",
    "all_test_logits = tf.concat(all_test_logits, axis=0)\n",
    "test_pred_ids = tf.argmax(all_test_logits, axis=1).numpy()\n",
    "\n",
    "print(\"Test predictions shape:\", test_pred_ids.shape)\n",
    "print(\"First 10 predictions:\", test_pred_ids[:10])\n",
    "\n",
    "# Build submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"PhraseId\": test_df[\"PhraseId\"],\n",
    "    \"Sentiment\": test_pred_ids,\n",
    "})\n",
    "\n",
    "submission.head()\n"
   ],
   "id": "c4f28ed9d79257e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save to CSV\n",
    "submission_file = \"submission_distilbert_tf.csv\"\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Saved submission to {submission_file}\")\n",
    "\n"
   ],
   "id": "db227b244df02162"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
